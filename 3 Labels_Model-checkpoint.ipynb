{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05be8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\Downloads\\\\balanced_processed_covid19_tweets_copy.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8685a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>case death today according worldometers anothe...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>nipostngn impose stupid inconsiderate charge l...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>sashadarapper bitch choked netflix go get job ...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>sad ugly triage texas covid tragedy unfold way</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>republican ignore million american threat evic...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment\n",
       "2332  case death today according worldometers anothe...     Anger\n",
       "2333  nipostngn impose stupid inconsiderate charge l...     Anger\n",
       "2334  sashadarapper bitch choked netflix go get job ...     Anger\n",
       "2335     sad ugly triage texas covid tragedy unfold way     Anger\n",
       "2336  republican ignore million american threat evic...     Anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd14b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text sentiment\n",
      "0     covid change work general recruiting specifica...   Neutral\n",
      "1     wear face covering shopping includes visit loc...   Neutral\n",
      "2     order logo graphicdesigner logodesign logodesi...   Neutral\n",
      "3     rajasthan government today started plasma bank...   Neutral\n",
      "4     nagaland police covid awareness city tower jun...   Neutral\n",
      "...                                                 ...       ...\n",
      "2332  case death today according worldometers anothe...  negative\n",
      "2333  nipostngn impose stupid inconsiderate charge l...  negative\n",
      "2334  sashadarapper bitch choked netflix go get job ...  negative\n",
      "2335     sad ugly triage texas covid tragedy unfold way  negative\n",
      "2336  republican ignore million american threat evic...  negative\n",
      "\n",
      "[2337 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "replacement_map = {\n",
    "    'Joy': 'positive',\n",
    "    'Fear': 'negative',\n",
    "    'Sad': 'negative',\n",
    "    'Anger': 'negative'\n",
    "}\n",
    "\n",
    "# Replace values in 'sentiment' column\n",
    "df['sentiment'].replace(replacement_map, inplace=True)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e90a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    457\n",
      "Neutral     457\n",
      "negative    457\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Find the minimum count among all sentiments\n",
    "min_count = sentiment_counts.min()\n",
    "\n",
    "# Filter rows for each sentiment category\n",
    "positive_rows = df[df['sentiment'] == 'positive']\n",
    "neutral_rows = df[df['sentiment'] == 'Neutral']\n",
    "negative_rows = df[df['sentiment'] == 'negative']\n",
    "\n",
    "# Sample rows if the count is greater than the minimum count\n",
    "if len(positive_rows) > min_count:\n",
    "    positive_rows = positive_rows.sample(n=min_count, random_state=42)\n",
    "if len(neutral_rows) > min_count:\n",
    "    neutral_rows = neutral_rows.sample(n=min_count, random_state=42)\n",
    "if len(negative_rows) > min_count:\n",
    "    negative_rows = negative_rows.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled rows\n",
    "balanced_df = pd.concat([positive_rows, neutral_rows, negative_rows])\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the balanced DataFrame\n",
    "print(balanced_df['sentiment'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6f018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text sentiment\n",
      "0     sound like plandemic get second life here thre...  positive\n",
      "1              latest islam islamism berkleyforum covid   Neutral\n",
      "2     staysafe summertime covid challenge put much s...  positive\n",
      "3     dear realdonaldtrump covid vaccine go market p...  positive\n",
      "4     wash hand clean hand save life download free g...  positive\n",
      "...                                                 ...       ...\n",
      "1366  badbitchinaz id pay republican leave america v...  negative\n",
      "1367  washington demanding australian government par...  negative\n",
      "1368  toonceslives resisting bitch face resist covid...  negative\n",
      "1369  tussfc told trial consist people getting covid...   Neutral\n",
      "1370         nypost high temp humidity kill covid virus  negative\n",
      "\n",
      "[1371 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13133dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911573af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels mapping\n",
    "label_mapping = {'positive': 0, 'Neutral': 1, 'negative': 2}\n",
    "\n",
    "# Convert sentiment labels to numerical labels\n",
    "df['sentiment'] = df['sentiment'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81890e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sound like plandemic get second life here thre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest islam islamism berkleyforum covid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>staysafe summertime covid challenge put much s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear realdonaldtrump covid vaccine go market p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wash hand clean hand save life download free g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  sound like plandemic get second life here thre...          0\n",
       "1           latest islam islamism berkleyforum covid          1\n",
       "2  staysafe summertime covid challenge put much s...          0\n",
       "3  dear realdonaldtrump covid vaccine go market p...          0\n",
       "4  wash hand clean hand save life download free g...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4247c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = df['text'].tolist()\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  \n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert TF-IDF features to an array\n",
    "tfidf_features = tfidf_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06219683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load GloVe word vectors\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        word_to_vec = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype='float32')\n",
    "            word_to_vec[word] = vec\n",
    "    return word_to_vec\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"C:\\\\Users\\\\lenovo\\\\My_Folders\\\\7th_sem\\\\IT350\\\\IT350_Project\\\\glove.6B.100d.txt\")\n",
    "\n",
    "# Convert tweets to GloVe embeddings\n",
    "def get_average_embedding(tweet, word_embeddings):\n",
    "    words = tweet.split()\n",
    "    embeddings = [word_embeddings.get(word, np.zeros(100)) for word in words]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Apply the embedding function to the 'text' column\n",
    "glove_features = df['text'].apply(lambda x: get_average_embedding(x, glove_embeddings))\n",
    "\n",
    "# Now, 'glove_features' contains the GloVe embeddings for each tweet\n",
    "# It's a Series of NumPy arrays\n",
    "\n",
    "# Convert 'glove_features' to a NumPy array\n",
    "glove_features = np.vstack(glove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558e9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize tweets\n",
    "tokenized_tweets = [word_tokenize(tweet.lower()) for tweet in df['text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to get the word embedding for a tweet\n",
    "def get_average_word2vec_embedding(tweet, word2vec_model):\n",
    "    words = word_tokenize(tweet.lower())\n",
    "    embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(100)\n",
    "\n",
    "# Apply the embedding function to the 'text' column\n",
    "word2vec_features = df['text'].apply(lambda x: get_average_word2vec_embedding(x, word2vec_model))\n",
    "\n",
    "# Convert 'word2vec_features' to a NumPy array\n",
    "word2vec_features = np.vstack(word2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84e69be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Train a FastText model\n",
    "model = FastText(sentences=df['text'].apply(lambda x: simple_preprocess(x)), vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to get the FastText embedding for a sentence\n",
    "def get_fasttext_embedding(sentence):\n",
    "    words = simple_preprocess(sentence)\n",
    "    return sum([model.wv[word] for word in words if word in model.wv])\n",
    "\n",
    "# Apply the function to your DataFrame and get the embeddings separately\n",
    "fasttext_embeddings = df['text'].apply(get_fasttext_embedding)\n",
    "\n",
    "# Now, fasttext_embeddings is a Series containing FastText-like embeddings for each tweet.\n",
    "# Each element of the series is a numpy array representing the embedding.\n",
    "fasttext_embeddings = np.vstack(fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f9627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>glove_features</th>\n",
       "      <th>word2vec_features</th>\n",
       "      <th>fasttext_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sound like plandemic get second life here thre...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.16007492433373743, 0.3628024634403678, 0.3...</td>\n",
       "      <td>[0.0003457685234025121, 0.006037031300365925, ...</td>\n",
       "      <td>[-1.0274704694747925, 2.6667635440826416, -3.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest islam islamism berkleyforum covid</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.09475399851799012, 0.10903059989213944, 0.0...</td>\n",
       "      <td>[-0.001335539622232318, 0.008357246406376362, ...</td>\n",
       "      <td>[-0.2983933985233307, 0.7834877967834473, -1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>staysafe summertime covid challenge put much s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.5785987173475471, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.3035103985323356, 0.22403684530693752, 0.2...</td>\n",
       "      <td>[-0.003014534479007125, 0.0006508661899715662,...</td>\n",
       "      <td>[-1.0606491565704346, 2.7465343475341797, -3.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear realdonaldtrump covid vaccine go market p...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0025485828518867493, 0.12183674859503905, 0...</td>\n",
       "      <td>[-0.0010916515020653605, 0.0020927099976688623...</td>\n",
       "      <td>[-1.1256067752838135, 2.9913766384124756, -3.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wash hand clean hand save life download free g...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.18380731344223022, 0.11013955622911453, 0....</td>\n",
       "      <td>[0.0009444393217563629, 0.0031865646596997976,...</td>\n",
       "      <td>[-0.9953127503395081, 2.553715229034424, -3.34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  sound like plandemic get second life here thre...          0   \n",
       "1           latest islam islamism berkleyforum covid          1   \n",
       "2  staysafe summertime covid challenge put much s...          0   \n",
       "3  dear realdonaldtrump covid vaccine go market p...          0   \n",
       "4  wash hand clean hand save life download free g...          0   \n",
       "\n",
       "                                      tfidf_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.5785987173475471, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      glove_features  \\\n",
       "0  [-0.16007492433373743, 0.3628024634403678, 0.3...   \n",
       "1  [0.09475399851799012, 0.10903059989213944, 0.0...   \n",
       "2  [-0.3035103985323356, 0.22403684530693752, 0.2...   \n",
       "3  [0.0025485828518867493, 0.12183674859503905, 0...   \n",
       "4  [-0.18380731344223022, 0.11013955622911453, 0....   \n",
       "\n",
       "                                   word2vec_features  \\\n",
       "0  [0.0003457685234025121, 0.006037031300365925, ...   \n",
       "1  [-0.001335539622232318, 0.008357246406376362, ...   \n",
       "2  [-0.003014534479007125, 0.0006508661899715662,...   \n",
       "3  [-0.0010916515020653605, 0.0020927099976688623...   \n",
       "4  [0.0009444393217563629, 0.0031865646596997976,...   \n",
       "\n",
       "                                 fasttext_embeddings  \n",
       "0  [-1.0274704694747925, 2.6667635440826416, -3.4...  \n",
       "1  [-0.2983933985233307, 0.7834877967834473, -1.0...  \n",
       "2  [-1.0606491565704346, 2.7465343475341797, -3.5...  \n",
       "3  [-1.1256067752838135, 2.9913766384124756, -3.8...  \n",
       "4  [-0.9953127503395081, 2.553715229034424, -3.34...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tfidf_features']=tfidf_features.tolist()\n",
    "df['glove_features']=glove_features.tolist()\n",
    "df['word2vec_features']=word2vec_features.tolist()\n",
    "df['fasttext_embeddings']=fasttext_embeddings.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa093eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Assuming your data is in a DataFrame called 'df'\n",
    "\n",
    "# # Combine all features into one array\n",
    "# X_tfidf = np.array(df['tfidf_features'].tolist())\n",
    "# X_glove = np.array(df['glove_features'].tolist())\n",
    "# X_word2vec = np.array(df['word2vec_features'].tolist())\n",
    "# X_fasttext = np.array(df['fasttext_embeddings'].tolist())\n",
    "\n",
    "# # Concatenate features\n",
    "# X = np.concatenate((X_tfidf, X_glove, X_word2vec, X_fasttext), axis=1)\n",
    "\n",
    "# # Assuming you have a 'sentiment' column in your DataFrame\n",
    "# y = np.array(df['sentiment'])\n",
    "\n",
    "# # Convert labels to one-hot encoded vectors\n",
    "# y = to_categorical(y, num_classes=3)  # 3 classes in total\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Pad sequences individually\n",
    "# max_len = 100  # Define your maximum sequence length\n",
    "# X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')  \n",
    "# X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# # Build GRU model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=X.shape[1], output_dim=128, input_length=max_len))  # Adjust output_dim as needed\n",
    "# model.add(GRU(64, return_sequences=True))  # Adjust GRU units as needed\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(32, activation='relu'))  # Additional dense layer\n",
    "# model.add(Dense(3, activation='softmax'))  # Output layer with 3 classes\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34aef23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 28ms/step - loss: 1.3496 - accuracy: 0.3750 - val_loss: 1.0593 - val_accuracy: 0.4182\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 1.1228 - accuracy: 0.4325 - val_loss: 1.0365 - val_accuracy: 0.4473\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 1.0373 - accuracy: 0.4699 - val_loss: 1.0272 - val_accuracy: 0.4655\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.9966 - accuracy: 0.5064 - val_loss: 0.9928 - val_accuracy: 0.5200\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.9640 - accuracy: 0.5228 - val_loss: 0.9731 - val_accuracy: 0.5273\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.9328 - accuracy: 0.5429 - val_loss: 0.9499 - val_accuracy: 0.5345\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.8856 - accuracy: 0.5958 - val_loss: 0.9253 - val_accuracy: 0.5564\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.8763 - accuracy: 0.5958 - val_loss: 0.8981 - val_accuracy: 0.5891\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.8334 - accuracy: 0.6214 - val_loss: 0.9061 - val_accuracy: 0.5891\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.7913 - accuracy: 0.6505 - val_loss: 0.8886 - val_accuracy: 0.6109\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.7336 - accuracy: 0.6743 - val_loss: 0.8693 - val_accuracy: 0.6218\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.7149 - accuracy: 0.6825 - val_loss: 0.8335 - val_accuracy: 0.6218\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.7139 - accuracy: 0.6971 - val_loss: 0.8493 - val_accuracy: 0.6364\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.6519 - accuracy: 0.7153 - val_loss: 0.8504 - val_accuracy: 0.6509\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.6343 - accuracy: 0.7400 - val_loss: 0.8607 - val_accuracy: 0.6364\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5844 - accuracy: 0.7482 - val_loss: 0.8772 - val_accuracy: 0.6255\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5650 - accuracy: 0.7755 - val_loss: 0.8833 - val_accuracy: 0.6218\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5434 - accuracy: 0.7801 - val_loss: 0.8943 - val_accuracy: 0.6218\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5635 - accuracy: 0.7673 - val_loss: 0.8997 - val_accuracy: 0.6291\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5381 - accuracy: 0.7865 - val_loss: 0.9459 - val_accuracy: 0.5891\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4716 - accuracy: 0.8193 - val_loss: 0.9211 - val_accuracy: 0.6218\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4403 - accuracy: 0.8157 - val_loss: 0.9689 - val_accuracy: 0.6255\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4702 - accuracy: 0.8166 - val_loss: 0.9407 - val_accuracy: 0.6145\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4213 - accuracy: 0.8358 - val_loss: 0.9807 - val_accuracy: 0.6291\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4061 - accuracy: 0.8403 - val_loss: 0.9718 - val_accuracy: 0.6327\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3950 - accuracy: 0.8376 - val_loss: 1.0309 - val_accuracy: 0.6473\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3863 - accuracy: 0.8604 - val_loss: 1.1442 - val_accuracy: 0.6436\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4057 - accuracy: 0.8385 - val_loss: 1.0200 - val_accuracy: 0.6473\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3537 - accuracy: 0.8668 - val_loss: 1.0623 - val_accuracy: 0.6509\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.3348 - accuracy: 0.8805 - val_loss: 1.1120 - val_accuracy: 0.6473\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2937 - accuracy: 0.8914 - val_loss: 1.1799 - val_accuracy: 0.6400\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.3154 - accuracy: 0.8759 - val_loss: 1.1949 - val_accuracy: 0.6182\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.3217 - accuracy: 0.8750 - val_loss: 1.1632 - val_accuracy: 0.6473\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2548 - accuracy: 0.9033 - val_loss: 1.2368 - val_accuracy: 0.6364\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2558 - accuracy: 0.9033 - val_loss: 1.2872 - val_accuracy: 0.6109\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2536 - accuracy: 0.9115 - val_loss: 1.2650 - val_accuracy: 0.6364\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2605 - accuracy: 0.9024 - val_loss: 1.2925 - val_accuracy: 0.6436\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2304 - accuracy: 0.9097 - val_loss: 1.3210 - val_accuracy: 0.6109\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2305 - accuracy: 0.9069 - val_loss: 1.3479 - val_accuracy: 0.6182\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2310 - accuracy: 0.9170 - val_loss: 1.3428 - val_accuracy: 0.6436\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2525 - accuracy: 0.9078 - val_loss: 1.2769 - val_accuracy: 0.6400\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2743 - accuracy: 0.8960 - val_loss: 1.2388 - val_accuracy: 0.6545\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2737 - accuracy: 0.9051 - val_loss: 1.3580 - val_accuracy: 0.6073\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2546 - accuracy: 0.9115 - val_loss: 1.2418 - val_accuracy: 0.6400\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2164 - accuracy: 0.9224 - val_loss: 1.3837 - val_accuracy: 0.6327\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.1973 - accuracy: 0.9243 - val_loss: 1.3602 - val_accuracy: 0.6400\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.1773 - accuracy: 0.9370 - val_loss: 1.4378 - val_accuracy: 0.6291\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2027 - accuracy: 0.9243 - val_loss: 1.4244 - val_accuracy: 0.6182\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.1582 - accuracy: 0.9325 - val_loss: 1.5070 - val_accuracy: 0.6291\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.1446 - accuracy: 0.9434 - val_loss: 1.5395 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23eb56f6a60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming your data is in a DataFrame called 'df'\n",
    "\n",
    "# Combine all features into one array\n",
    "X_tfidf = np.array(df['tfidf_features'].tolist())\n",
    "X_glove = np.array(df['glove_features'].tolist())\n",
    "X_word2vec = np.array(df['word2vec_features'].tolist())\n",
    "X_fasttext = np.array(df['fasttext_embeddings'].tolist())\n",
    "\n",
    "# Concatenate features\n",
    "X = np.concatenate((X_tfidf, X_glove, X_word2vec, X_fasttext), axis=1)\n",
    "# X = np.concatenate((X_glove, X_word2vec), axis=1)\n",
    "\n",
    "# Assuming you have a 'sentiment' column in your DataFrame\n",
    "y = np.array(df['sentiment'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))  # Output layer with 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec6d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28bf8108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos :  367\n",
      "Neutral :  365\n",
      "Neg :  364\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "n=0\n",
    "neg=0\n",
    "for x in y_train:\n",
    "    if(x==0):\n",
    "        p+=1\n",
    "    if(x==1):\n",
    "        n+=1\n",
    "    if(x==2):\n",
    "        neg+=1\n",
    "print(\"Pos : \",p)\n",
    "print(\"Neutral : \",n)\n",
    "print(\"Neg : \",neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f128e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 0, 1,\n",
       "       0, 0, 0, 2, 0, 1, 2, 2, 0, 1, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 2,\n",
       "       2, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 1,\n",
       "       1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n",
       "       2, 1, 1, 1, 2, 0, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 2, 1,\n",
       "       1, 1, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0,\n",
       "       0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 0, 2,\n",
       "       0, 2, 1, 2, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1,\n",
       "       2, 2, 0, 1, 1, 2, 0, 0, 1, 2, 2, 1, 1, 0, 1, 1, 2, 2, 2, 0, 2, 1,\n",
       "       0, 1, 1, 0, 1, 2, 2, 2, 1, 2, 0, 0, 2, 2, 0, 2, 1, 0, 1, 1, 2, 2,\n",
       "       2, 0, 0, 2, 2, 1, 2, 2, 1, 0, 1, 1, 2, 0, 2, 1, 0, 1, 1, 2, 1, 1,\n",
       "       1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 0, 2,\n",
       "       2, 0, 0, 1, 1, 2, 0, 2, 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "702c3439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos :  90\n",
      "Neutral :  92\n",
      "Neg :  93\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "n=0\n",
    "neg=0\n",
    "for x in y_test:\n",
    "    if(x==0):\n",
    "        p+=1\n",
    "    if(x==1):\n",
    "        n+=1\n",
    "    if(x==2):\n",
    "        neg+=1\n",
    "print(\"Pos : \",p)\n",
    "print(\"Neutral : \",n)\n",
    "print(\"Neg : \",neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d25c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
