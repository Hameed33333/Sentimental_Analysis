{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f46f3e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smelled scent hand sanitizers today someone pa...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey yankee yankeespr mlb wouldnt made sense pl...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diane wdunlap realdonaldtrump trump never clai...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brookbanktv one gift covid give appreciation s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>july medium bulletin novel coronavirusupdates ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_label\n",
       "0  smelled scent hand sanitizers today someone pa...            Fear\n",
       "1  hey yankee yankeespr mlb wouldnt made sense pl...             Joy\n",
       "2  diane wdunlap realdonaldtrump trump never clai...         Neutral\n",
       "3  brookbanktv one gift covid give appreciation s...         Neutral\n",
       "4  july medium bulletin novel coronavirusupdates ...         Neutral"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\My_Folders\\\\7th_sem\\\\IT350\\\\IT350_Project\\\\processed_covid19_tweets_copy.csv\")\n",
    "\n",
    "# Select the 10th and 14th columns\n",
    "selected_columns = df.iloc[:, [9, 13]]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "df = pd.DataFrame(selected_columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "df.head()  # You can adjust the number of rows to display if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "023933d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    smelled scent hand sanitizers today someone pa...\n",
       "1    hey yankee yankeespr mlb wouldnt made sense pl...\n",
       "2    diane wdunlap realdonaldtrump trump never clai...\n",
       "3    brookbanktv one gift covid give appreciation s...\n",
       "4    july medium bulletin novel coronavirusupdates ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9805be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = df['text'].tolist()\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # You can adjust max_features as needed\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert TF-IDF features to an array\n",
    "tfidf_features = tfidf_features.toarray()\n",
    "\n",
    "# Now you can use tfidf_features for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69a31d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca845276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.74127811 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.46002104 0.63581888 0.         ... 0.59421808 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26ab9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.67119801 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.74127811 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a03d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with columns 'text' and 'sentiment_label'\n",
    "# Load the data if not already done\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Load GloVe word vectors\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        word_to_vec = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype='float32')\n",
    "            word_to_vec[word] = vec\n",
    "    return word_to_vec\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"C:\\\\Users\\\\lenovo\\\\My_Folders\\\\7th_sem\\\\IT350\\\\IT350_Project\\\\glove.6B.100d.txt\")\n",
    "\n",
    "# Convert tweets to GloVe embeddings\n",
    "def get_average_embedding(tweet, word_embeddings):\n",
    "    words = tweet.split()\n",
    "    embeddings = [word_embeddings.get(word, np.zeros(100)) for word in words]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Apply the embedding function to the 'text' column\n",
    "glove_features = df['text'].apply(lambda x: get_average_embedding(x, glove_embeddings))\n",
    "\n",
    "# Now, 'glove_features' contains the GloVe embeddings for each tweet\n",
    "# It's a Series of NumPy arrays\n",
    "\n",
    "# Convert 'glove_features' to a NumPy array\n",
    "glove_features = np.vstack(glove_features)\n",
    "\n",
    "# Now you can use glove_features for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86b02137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.21700031e-02, -6.51182977e-02,  3.29189979e-02, -1.39735997e-01,\n",
       "       -4.61089969e-02,  8.55841556e-02,  7.35783007e-02, -1.26873999e-01,\n",
       "        1.67517221e-01,  1.42497006e-01,  1.63984404e-01,  3.19485799e-01,\n",
       "        8.26836031e-02, -1.38394948e-02, -5.70779016e-02, -9.13355484e-02,\n",
       "        8.53503006e-02,  2.37384103e-01, -4.14313996e-01,  1.98311999e-01,\n",
       "        2.16264099e-01, -5.04609481e-02,  1.08742905e-01, -6.62582045e-02,\n",
       "        6.37069985e-02, -2.51829007e-02, -4.50118013e-02, -1.49618911e-01,\n",
       "       -1.21529800e-01, -3.91582997e-01,  1.98995899e-01,  3.37842198e-01,\n",
       "        1.01086299e-01,  1.26345998e-01,  9.44960006e-02, -5.28789982e-02,\n",
       "       -8.43998026e-02, -3.03883072e-02, -4.64772996e-02,  6.62606016e-02,\n",
       "       -3.29755294e-01,  1.60255931e-01,  9.93152896e-02, -1.69020002e-01,\n",
       "        5.76875962e-02, -1.22474003e-01,  1.56499501e-01, -1.81320406e-01,\n",
       "        6.31553009e-02, -3.86708906e-01,  4.80103385e-02,  1.30737402e-01,\n",
       "        2.58514842e-01,  4.28406991e-01,  5.30392051e-02, -1.10972599e+00,\n",
       "       -8.93340006e-02,  3.41296569e-04,  4.83078998e-01,  2.04999802e-01,\n",
       "        4.00193006e-02,  2.71140152e-01, -4.78860177e-03, -1.37897699e-01,\n",
       "        3.73085998e-01, -1.92904999e-01,  1.99624695e-01,  3.46864893e-01,\n",
       "       -1.33529102e-01, -9.15167026e-02, -2.96850003e-02,  3.32629979e-02,\n",
       "       -2.44121848e-01, -7.27382984e-02, -5.60806021e-02,  6.61984988e-02,\n",
       "        9.16896038e-02, -1.50163398e-01, -6.64131704e-01,  3.28211001e-01,\n",
       "        1.90731001e-01, -4.78413008e-02,  3.22892006e-02, -1.88557702e-01,\n",
       "       -6.35642483e-01, -2.31116000e-01,  8.24851790e-03,  1.46026002e-01,\n",
       "        2.70499984e-02, -1.48010197e-01, -1.40788972e-02, -1.33283695e-01,\n",
       "        1.58886100e-01, -3.78054006e-02,  2.10134052e-02,  9.78564985e-02,\n",
       "       -7.12703027e-02, -1.10540198e-01,  2.88730025e-02,  4.91645008e-02])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8781f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with columns 'text' and 'sentiment_label'\n",
    "# Make sure you have NLTK installed for tokenization\n",
    "\n",
    "# Tokenize tweets\n",
    "tokenized_tweets = [word_tokenize(tweet.lower()) for tweet in df['text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to get the word embedding for a tweet\n",
    "def get_average_word2vec_embedding(tweet, word2vec_model):\n",
    "    words = word_tokenize(tweet.lower())\n",
    "    embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(100)\n",
    "\n",
    "# Apply the embedding function to the 'text' column\n",
    "word2vec_features = df['text'].apply(lambda x: get_average_word2vec_embedding(x, word2vec_model))\n",
    "\n",
    "# Convert 'word2vec_features' to a NumPy array\n",
    "word2vec_features = np.vstack(word2vec_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "958bee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fc0613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame df is already loaded with columns 'text' and 'sentiment_label'\n",
    "# If not, load your data into a DataFrame like this:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Train a FastText model\n",
    "model = FastText(sentences=df['text'].apply(lambda x: simple_preprocess(x)), vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to get the FastText embedding for a sentence\n",
    "def get_fasttext_embedding(sentence):\n",
    "    words = simple_preprocess(sentence)\n",
    "    return sum([model.wv[word] for word in words if word in model.wv])\n",
    "\n",
    "# Apply the function to your DataFrame and get the embeddings separately\n",
    "fasttext_embeddings = df['text'].apply(get_fasttext_embedding)\n",
    "\n",
    "# Now, fasttext_embeddings is a Series containing FastText-like embeddings for each tweet.\n",
    "# Each element of the series is a numpy array representing the embedding.\n",
    "fasttext_embeddings = np.vstack(fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d8a2f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83efa1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85125977,  2.3672068 , -3.7065296 , ..., -2.1992464 ,\n",
       "         0.33533943,  6.70496   ],\n",
       "       [ 0.61042166,  1.8772321 , -2.9301038 , ..., -1.7417018 ,\n",
       "         0.25146404,  5.327695  ],\n",
       "       [ 0.6706051 ,  2.0564547 , -3.0384996 , ..., -1.9044957 ,\n",
       "         0.2619517 ,  5.6500106 ],\n",
       "       ...,\n",
       "       [ 0.8076517 ,  2.4414952 , -3.7630522 , ..., -2.2398782 ,\n",
       "         0.31911156,  6.8522067 ],\n",
       "       [ 0.5976361 ,  1.8744097 , -2.8465042 , ..., -1.7316846 ,\n",
       "         0.2526119 ,  5.2284184 ],\n",
       "       [-0.25538453,  5.500887  , -2.181042  , ..., -5.486907  ,\n",
       "        -0.5074955 , 10.339474  ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6de935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tfidf_features']=tfidf_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acbdf15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>glove_features</th>\n",
       "      <th>word2vec_features</th>\n",
       "      <th>fasttext_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·èâ·é•‚òª’¨ÍÇÖœÆ</td>\n",
       "      <td>astroworld</td>\n",
       "      <td>wednesday addams as a disney princess keepin i...</td>\n",
       "      <td>2017-05-26 05:46:42</td>\n",
       "      <td>624</td>\n",
       "      <td>950</td>\n",
       "      <td>18775</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:21</td>\n",
       "      <td>smelled scent hand sanitizers today someone pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.04764946550130844, 0.18273480236530304, 0.3...</td>\n",
       "      <td>[-0.0027747941203415394, 0.01915343850851059, ...</td>\n",
       "      <td>[0.7700698971748352, 2.5102710723876953, -3.53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom Basile üá∫üá∏</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Husband, Father, Columnist &amp; Commentator. Auth...</td>\n",
       "      <td>2009-04-16 20:06:23</td>\n",
       "      <td>2253</td>\n",
       "      <td>1677</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-25 12:27:17</td>\n",
       "      <td>hey yankee yankeespr mlb wouldnt made sense pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.26341699808835983, 0.23652300536632537, 0.5...</td>\n",
       "      <td>[-0.005607889499515295, 0.002932238392531872, ...</td>\n",
       "      <td>[0.5457693338394165, 1.9875335693359375, -2.79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time4fisticuffs</td>\n",
       "      <td>Pewee Valley, KY</td>\n",
       "      <td>#Christian #Catholic #Conservative #Reagan #Re...</td>\n",
       "      <td>2009-02-28 18:57:41</td>\n",
       "      <td>9275</td>\n",
       "      <td>9525</td>\n",
       "      <td>7254</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:14</td>\n",
       "      <td>diane wdunlap realdonaldtrump trump never clai...</td>\n",
       "      <td>['COVID19']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.09217000305652619, -0.06511829774826765, 0....</td>\n",
       "      <td>[-0.010386739857494831, 0.031815286725759506, ...</td>\n",
       "      <td>[0.5927481055259705, 2.184103012084961, -2.875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ethel mertz</td>\n",
       "      <td>Stuck in the Middle</td>\n",
       "      <td>#Browns #Indians #ClevelandProud #[]_[] #Cavs ...</td>\n",
       "      <td>2019-03-07 01:45:06</td>\n",
       "      <td>197</td>\n",
       "      <td>987</td>\n",
       "      <td>1488</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:10</td>\n",
       "      <td>brookbanktv one gift covid give appreciation s...</td>\n",
       "      <td>['COVID19']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.019209399819374084, 0.3748170003294945, 0.2...</td>\n",
       "      <td>[-0.013840138912200928, 0.036234788596630096, ...</td>\n",
       "      <td>[0.6640568971633911, 2.3808679580688477, -3.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIPR-J&amp;K</td>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>üñäÔ∏èOfficial Twitter handle of Department of Inf...</td>\n",
       "      <td>2017-02-12 06:45:15</td>\n",
       "      <td>101009</td>\n",
       "      <td>168</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-25 12:27:08</td>\n",
       "      <td>july medium bulletin novel coronavirusupdates ...</td>\n",
       "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.17231430485844612, -0.08096492665354163, -...</td>\n",
       "      <td>[-0.004336285870522261, 0.03020189143717289, 0...</td>\n",
       "      <td>[0.43173494935035706, 1.7645716667175293, -2.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name         user_location  \\\n",
       "0           ·èâ·é•‚òª’¨ÍÇÖœÆ            astroworld   \n",
       "1    Tom Basile üá∫üá∏          New York, NY   \n",
       "2  Time4fisticuffs      Pewee Valley, KY   \n",
       "3      ethel mertz  Stuck in the Middle    \n",
       "4         DIPR-J&K     Jammu and Kashmir   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  wednesday addams as a disney princess keepin i...  2017-05-26 05:46:42   \n",
       "1  Husband, Father, Columnist & Commentator. Auth...  2009-04-16 20:06:23   \n",
       "2  #Christian #Catholic #Conservative #Reagan #Re...  2009-02-28 18:57:41   \n",
       "3  #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  2019-03-07 01:45:06   \n",
       "4  üñäÔ∏èOfficial Twitter handle of Department of Inf...  2017-02-12 06:45:15   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             624           950            18775          False   \n",
       "1            2253          1677               24           True   \n",
       "2            9275          9525             7254          False   \n",
       "3             197           987             1488          False   \n",
       "4          101009           168              101          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-07-25 12:27:21  smelled scent hand sanitizers today someone pa...   \n",
       "1  2020-07-25 12:27:17  hey yankee yankeespr mlb wouldnt made sense pl...   \n",
       "2  2020-07-25 12:27:14  diane wdunlap realdonaldtrump trump never clai...   \n",
       "3  2020-07-25 12:27:10  brookbanktv one gift covid give appreciation s...   \n",
       "4  2020-07-25 12:27:08  july medium bulletin novel coronavirusupdates ...   \n",
       "\n",
       "                            hashtags               source  is_retweet  \\\n",
       "0                                NaN   Twitter for iPhone       False   \n",
       "1                                NaN  Twitter for Android       False   \n",
       "2                        ['COVID19']  Twitter for Android       False   \n",
       "3                        ['COVID19']   Twitter for iPhone       False   \n",
       "4  ['CoronaVirusUpdates', 'COVID19']  Twitter for Android       False   \n",
       "\n",
       "   sentiment_label                                     tfidf_features  \\\n",
       "0                3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1                0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2                1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3                1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4                1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      glove_features  \\\n",
       "0  [0.04764946550130844, 0.18273480236530304, 0.3...   \n",
       "1  [0.26341699808835983, 0.23652300536632537, 0.5...   \n",
       "2  [0.09217000305652619, -0.06511829774826765, 0....   \n",
       "3  [0.019209399819374084, 0.3748170003294945, 0.2...   \n",
       "4  [-0.17231430485844612, -0.08096492665354163, -...   \n",
       "\n",
       "                                   word2vec_features  \\\n",
       "0  [-0.0027747941203415394, 0.01915343850851059, ...   \n",
       "1  [-0.005607889499515295, 0.002932238392531872, ...   \n",
       "2  [-0.010386739857494831, 0.031815286725759506, ...   \n",
       "3  [-0.013840138912200928, 0.036234788596630096, ...   \n",
       "4  [-0.004336285870522261, 0.03020189143717289, 0...   \n",
       "\n",
       "                                 fasttext_embeddings  \n",
       "0  [0.7700698971748352, 2.5102710723876953, -3.53...  \n",
       "1  [0.5457693338394165, 1.9875335693359375, -2.79...  \n",
       "2  [0.5927481055259705, 2.184103012084961, -2.875...  \n",
       "3  [0.6640568971633911, 2.3808679580688477, -3.19...  \n",
       "4  [0.43173494935035706, 1.7645716667175293, -2.1...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_features']=glove_features.tolist()\n",
    "df['word2vec_features']=word2vec_features.tolist()\n",
    "df['fasttext_embeddings']=fasttext_embeddings.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5931790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smelled scent hand sanitizers today someone pa...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey yankee yankeespr mlb wouldnt made sense pl...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diane wdunlap realdonaldtrump trump never clai...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brookbanktv one gift covid give appreciation s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>july medium bulletin novel coronavirusupdates ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_label\n",
       "0  smelled scent hand sanitizers today someone pa...            Fear\n",
       "1  hey yankee yankeespr mlb wouldnt made sense pl...             Joy\n",
       "2  diane wdunlap realdonaldtrump trump never clai...         Neutral\n",
       "3  brookbanktv one gift covid give appreciation s...         Neutral\n",
       "4  july medium bulletin novel coronavirusupdates ...         Neutral"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b4a7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "756f6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels mapping\n",
    "label_mapping = {'Joy': 0, 'Neutral': 1, 'Sad': 2, 'Fear': 3, 'Anger': 4}\n",
    "\n",
    "# Convert sentiment labels to numerical labels\n",
    "df['sentiment_label'] = df['sentiment_label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d283c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smelled scent hand sanitizers today someone pa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey yankee yankeespr mlb wouldnt made sense pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diane wdunlap realdonaldtrump trump never clai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brookbanktv one gift covid give appreciation s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>july medium bulletin novel coronavirusupdates ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment_label\n",
       "0  smelled scent hand sanitizers today someone pa...                3\n",
       "1  hey yankee yankeespr mlb wouldnt made sense pl...                0\n",
       "2  diane wdunlap realdonaldtrump trump never clai...                1\n",
       "3  brookbanktv one gift covid give appreciation s...                1\n",
       "4  july medium bulletin novel coronavirusupdates ...                1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42e184f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 5ms/step - loss: 1.1195 - accuracy: 0.5427 - val_loss: 0.9505 - val_accuracy: 0.6417\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.9321 - accuracy: 0.6266 - val_loss: 0.8766 - val_accuracy: 0.6583\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.8367 - accuracy: 0.6865 - val_loss: 0.8252 - val_accuracy: 0.6750\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.7737 - accuracy: 0.6969 - val_loss: 0.8199 - val_accuracy: 0.6938\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.7215 - accuracy: 0.7208 - val_loss: 0.8127 - val_accuracy: 0.6875\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.7500 - val_loss: 0.8243 - val_accuracy: 0.6542\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.6201 - accuracy: 0.7719 - val_loss: 0.8420 - val_accuracy: 0.6917\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.5504 - accuracy: 0.7943 - val_loss: 0.8660 - val_accuracy: 0.6833\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.4983 - accuracy: 0.8198 - val_loss: 0.8837 - val_accuracy: 0.6792\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.4452 - accuracy: 0.8396 - val_loss: 0.9011 - val_accuracy: 0.6750\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8771 - val_loss: 0.9431 - val_accuracy: 0.6583\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.3275 - accuracy: 0.9005 - val_loss: 0.9652 - val_accuracy: 0.6667\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.2682 - accuracy: 0.9271 - val_loss: 1.0135 - val_accuracy: 0.6604\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.2268 - accuracy: 0.9406 - val_loss: 1.0567 - val_accuracy: 0.6583\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.1826 - accuracy: 0.9573 - val_loss: 1.1369 - val_accuracy: 0.6583\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9661 - val_loss: 1.2103 - val_accuracy: 0.6521\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.1229 - accuracy: 0.9750 - val_loss: 1.2819 - val_accuracy: 0.6625\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9844 - val_loss: 1.3940 - val_accuracy: 0.6604\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9880 - val_loss: 1.3751 - val_accuracy: 0.6521\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9922 - val_loss: 1.4557 - val_accuracy: 0.6562\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9927 - val_loss: 1.5289 - val_accuracy: 0.6521\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9964 - val_loss: 1.5425 - val_accuracy: 0.6562\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9974 - val_loss: 1.5747 - val_accuracy: 0.6479\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.9979 - val_loss: 1.6449 - val_accuracy: 0.6583\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9979 - val_loss: 1.6912 - val_accuracy: 0.6542\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.9995 - val_loss: 1.7135 - val_accuracy: 0.6562\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9990 - val_loss: 1.8049 - val_accuracy: 0.6562\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 0.9995 - val_loss: 1.8069 - val_accuracy: 0.6542\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 1.8464 - val_accuracy: 0.6604\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9995 - val_loss: 1.8802 - val_accuracy: 0.6583\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.8725 - val_accuracy: 0.6562\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9995 - val_loss: 1.9236 - val_accuracy: 0.6625\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9995 - val_loss: 1.9243 - val_accuracy: 0.6604\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9990 - val_loss: 1.9874 - val_accuracy: 0.6500\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 2.0641 - val_accuracy: 0.6479\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9990 - val_loss: 1.9925 - val_accuracy: 0.6562\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.0425 - val_accuracy: 0.6687\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.0591 - val_accuracy: 0.6646\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.1168 - val_accuracy: 0.6646\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.1241 - val_accuracy: 0.6604\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.1340 - val_accuracy: 0.6646\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.1537 - val_accuracy: 0.6583\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.1994 - val_accuracy: 0.6625\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.1913 - val_accuracy: 0.6625\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.2033 - val_accuracy: 0.6667\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.2240 - val_accuracy: 0.6625\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.2347 - val_accuracy: 0.6562\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.2482 - val_accuracy: 0.6604\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.2716 - val_accuracy: 0.6604\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.2804 - val_accuracy: 0.6646\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.3387 - accuracy: 0.6750\n",
      "Test Accuracy: 0.675000011920929\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Combine the extracted features\n",
    "combined_features = np.concatenate((tfidf_features, glove_features, word2vec_features), axis=1)\n",
    "\n",
    "# Define your labels and features\n",
    "labels = df['sentiment_label'].values\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels_encoded = to_categorical(labels)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ensemble model\n",
    "ensemble_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(300,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "ensemble_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = ensemble_model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = ensemble_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), predicted_labels)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# # Train the model and store the history\n",
    "# history = ensemble_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Access loss values from history\n",
    "# training_loss = history.history['loss']\n",
    "# validation_loss = history.history['val_loss']\n",
    "\n",
    "# # Create a plot for the loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(training_loss, label='Training Loss')\n",
    "# plt.plot(validation_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "544eb609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 41s 523ms/step - loss: 3.2895 - accuracy: 0.5213 - val_loss: 1.3033 - val_accuracy: 0.5383\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 39s 527ms/step - loss: 1.4288 - accuracy: 0.5238 - val_loss: 1.0538 - val_accuracy: 0.5383\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 40s 538ms/step - loss: 1.1030 - accuracy: 0.5238 - val_loss: 1.1055 - val_accuracy: 0.5383\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 40s 535ms/step - loss: 1.0780 - accuracy: 0.5238 - val_loss: 1.0427 - val_accuracy: 0.5383\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 40s 537ms/step - loss: 1.0520 - accuracy: 0.5238 - val_loss: 1.0383 - val_accuracy: 0.5383\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 41s 545ms/step - loss: 1.0503 - accuracy: 0.5238 - val_loss: 1.0399 - val_accuracy: 0.5383\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 41s 541ms/step - loss: 1.0498 - accuracy: 0.5238 - val_loss: 1.0368 - val_accuracy: 0.5383\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 41s 542ms/step - loss: 1.0488 - accuracy: 0.5238 - val_loss: 1.0346 - val_accuracy: 0.5383\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 40s 535ms/step - loss: 1.0489 - accuracy: 0.5238 - val_loss: 1.0341 - val_accuracy: 0.5383\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 41s 544ms/step - loss: 1.0481 - accuracy: 0.5238 - val_loss: 1.0334 - val_accuracy: 0.5383\n",
      "19/19 [==============================] - 3s 155ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "0.5383333333333333\n",
      "0.0\n",
      "0.3767750090285302\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, f1_score\n",
    "\n",
    "# Assuming you have a DataFrame named df\n",
    "\n",
    "# Extract features and labels\n",
    "X_tfidf = np.array(df['tfidf_features'].tolist())\n",
    "\n",
    "y = df['sentiment_label']\n",
    "\n",
    "# Reshape the data to have a third dimension\n",
    "X_tfidf = X_tfidf.reshape(X_tfidf.shape[0], X_tfidf.shape[1], 1)\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "# Repeat the above steps for other feature types\n",
    "\n",
    "# Define GRU model\n",
    "model_tfidf = Sequential([\n",
    "    GRU(128, input_shape=(X_tfidf.shape[1], X_tfidf.shape[2]), activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tfidf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_tfidf.fit(X_train_tfidf, y_train, epochs=10, validation_data=(X_test_tfidf, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_tfidf = np.argmax(model_tfidf.predict(X_test_tfidf), axis=-1)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf, average='weighted')\n",
    "mcc_tfidf = matthews_corrcoef(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf, average='weighted')\n",
    "\n",
    "print(y_pred_tfidf)\n",
    "print(accuracy_tfidf)\n",
    "print(mcc_tfidf)\n",
    "print(f1_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76e66b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 10s 106ms/step - loss: 1.3196 - accuracy: 0.5175 - val_loss: 1.0709 - val_accuracy: 0.5383\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 8s 108ms/step - loss: 1.0709 - accuracy: 0.5238 - val_loss: 1.0361 - val_accuracy: 0.5383\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 8s 101ms/step - loss: 1.0552 - accuracy: 0.5238 - val_loss: 1.0310 - val_accuracy: 0.5383\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 1.0480 - accuracy: 0.5238 - val_loss: 1.0317 - val_accuracy: 0.5383\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 8s 101ms/step - loss: 1.0485 - accuracy: 0.5238 - val_loss: 1.0272 - val_accuracy: 0.5383\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 1.0439 - accuracy: 0.5242 - val_loss: 1.0259 - val_accuracy: 0.5383\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0444 - accuracy: 0.5238 - val_loss: 1.0257 - val_accuracy: 0.5383\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 8s 104ms/step - loss: 1.0421 - accuracy: 0.5238 - val_loss: 1.0236 - val_accuracy: 0.5383\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 8s 102ms/step - loss: 1.0389 - accuracy: 0.5242 - val_loss: 1.0258 - val_accuracy: 0.5383\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 8s 107ms/step - loss: 1.0389 - accuracy: 0.5242 - val_loss: 1.0191 - val_accuracy: 0.5383\n",
      "19/19 [==============================] - 1s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, f1_score\n",
    "\n",
    "# Assuming you have a DataFrame named df\n",
    "\n",
    "# Extract features and labels\n",
    "X_glove = np.array(df['glove_features'].tolist())\n",
    "y = df['sentiment_label']\n",
    "\n",
    "# Reshape the data to have a third dimension\n",
    "X_glove = X_glove.reshape(X_glove.shape[0], X_glove.shape[1], 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_glove, X_test_glove, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define GRU model\n",
    "model_glove = Sequential([\n",
    "    GRU(128, input_shape=(X_glove.shape[1], X_glove.shape[2]), activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_glove.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_glove.fit(X_train_glove, y_train, epochs=10, validation_data=(X_test_glove, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_glove = np.argmax(model_glove.predict(X_test_glove), axis=-1)\n",
    "accuracy_glove = accuracy_score(y_test, y_pred_glove)\n",
    "recall_glove = recall_score(y_test, y_pred_glove, average='weighted')\n",
    "mcc_glove = matthews_corrcoef(y_test, y_pred_glove)\n",
    "f1_glove = f1_score(y_test, y_pred_glove, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29e6e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5383333333333333\n",
      "0.5383333333333333\n",
      "0.3767750090285302\n",
      "0.0\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "1801    1\n",
      "1190    1\n",
      "1817    1\n",
      "251     0\n",
      "2505    0\n",
      "       ..\n",
      "104     1\n",
      "2087    0\n",
      "599     1\n",
      "1756    1\n",
      "1323    1\n",
      "Name: sentiment_label, Length: 600, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_glove)\n",
    "print(recall_glove)\n",
    "print(f1_glove)\n",
    "print(mcc_glove)\n",
    "print(y_pred_glove)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1607689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 10s 107ms/step - loss: 1.3426 - accuracy: 0.5171 - val_loss: 1.1697 - val_accuracy: 0.5383\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 8s 104ms/step - loss: 1.0853 - accuracy: 0.5238 - val_loss: 1.0383 - val_accuracy: 0.5383\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0511 - accuracy: 0.5238 - val_loss: 1.0345 - val_accuracy: 0.5383\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 8s 107ms/step - loss: 1.0536 - accuracy: 0.5238 - val_loss: 1.0354 - val_accuracy: 0.5383\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0524 - accuracy: 0.5238 - val_loss: 1.0410 - val_accuracy: 0.5383\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 8s 104ms/step - loss: 1.0517 - accuracy: 0.5238 - val_loss: 1.0327 - val_accuracy: 0.5383\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 8s 104ms/step - loss: 1.0493 - accuracy: 0.5238 - val_loss: 1.0332 - val_accuracy: 0.5383\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 1.0480 - accuracy: 0.5238 - val_loss: 1.0324 - val_accuracy: 0.5383\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 1.0513 - accuracy: 0.5238 - val_loss: 1.0360 - val_accuracy: 0.5383\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0494 - accuracy: 0.5238 - val_loss: 1.0325 - val_accuracy: 0.5383\n",
      "19/19 [==============================] - 1s 34ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "0.5383333333333333\n",
      "0.3767750090285302\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "\n",
    "# Extract features and labels\n",
    "X_word2vec = np.array(df['word2vec_features'].tolist())\n",
    "\n",
    "X_word2vec = X_word2vec.reshape(X_word2vec.shape[0], X_word2vec.shape[1], 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_word2vec, X_test_word2vec, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define GRU model\n",
    "model_word2vec = Sequential([\n",
    "    GRU(128, input_shape=(X_word2vec.shape[1], X_word2vec.shape[2]), activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_word2vec.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_word2vec.fit(X_train_word2vec, y_train, epochs=10, validation_data=(X_test_word2vec, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_word2vec = np.argmax(model_word2vec.predict(X_test_word2vec), axis=-1)\n",
    "accuracy_word2vec = accuracy_score(y_test, y_pred_word2vec)\n",
    "recall_word2vec = recall_score(y_test, y_pred_word2vec, average='weighted')\n",
    "mcc_word2vec = matthews_corrcoef(y_test, y_pred_word2vec)\n",
    "f1_word2vec = f1_score(y_test, y_pred_word2vec, average='weighted')\n",
    "print(y_pred_word2vec)\n",
    "print(accuracy_word2vec)\n",
    "print(f1_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3688cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 10s 108ms/step - loss: 1.1508 - accuracy: 0.5100 - val_loss: 1.0525 - val_accuracy: 0.5383\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0468 - accuracy: 0.5179 - val_loss: 1.0378 - val_accuracy: 0.5383\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 8s 101ms/step - loss: 1.0334 - accuracy: 0.5188 - val_loss: 1.0181 - val_accuracy: 0.5467\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0232 - accuracy: 0.5296 - val_loss: 1.0183 - val_accuracy: 0.5500\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0191 - accuracy: 0.5325 - val_loss: 1.0164 - val_accuracy: 0.5417\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 8s 102ms/step - loss: 1.0193 - accuracy: 0.5346 - val_loss: 1.0164 - val_accuracy: 0.5300\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 8s 103ms/step - loss: 1.0124 - accuracy: 0.5333 - val_loss: 1.0014 - val_accuracy: 0.5450\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 1.0105 - accuracy: 0.5333 - val_loss: 1.0103 - val_accuracy: 0.5383\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 8s 102ms/step - loss: 1.0055 - accuracy: 0.5362 - val_loss: 0.9972 - val_accuracy: 0.5467\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 8s 108ms/step - loss: 1.0060 - accuracy: 0.5371 - val_loss: 0.9969 - val_accuracy: 0.5467\n",
      "19/19 [==============================] - 1s 33ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "0.5466666666666666\n",
      "0.39874021395575004\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "\n",
    "# Extract features and labels\n",
    "X_fasttext = np.array(df['fasttext_embeddings'].tolist())\n",
    "\n",
    "X_fasttext = X_fasttext.reshape(X_fasttext.shape[0], X_fasttext.shape[1], 1)\n",
    "# Split data into train and test sets\n",
    "X_train_fasttext, X_test_fasttext, y_train, y_test = train_test_split(X_fasttext, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define GRU model\n",
    "model_fasttext = Sequential([\n",
    "    GRU(128, input_shape=(X_fasttext.shape[1], X_fasttext.shape[2]), activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_fasttext.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_fasttext.fit(X_train_fasttext, y_train, epochs=10, validation_data=(X_test_fasttext, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_fasttext = np.argmax(model_fasttext.predict(X_test_fasttext), axis=-1)\n",
    "accuracy_fasttext = accuracy_score(y_test, y_pred_fasttext)\n",
    "recall_fasttext = recall_score(y_test, y_pred_fasttext, average='weighted')\n",
    "mcc_fasttext = matthews_corrcoef(y_test, y_pred_fasttext)\n",
    "f1_fasttext = f1_score(y_test, y_pred_fasttext, average='weighted')\n",
    "print(y_pred_fasttext)\n",
    "print(accuracy_fasttext)\n",
    "print(f1_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8590817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 64s 837ms/step - loss: 1.1522 - accuracy: 0.4921 - val_loss: 1.0478 - val_accuracy: 0.5383\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 62s 831ms/step - loss: 1.0459 - accuracy: 0.5192 - val_loss: 1.0249 - val_accuracy: 0.5383\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 62s 830ms/step - loss: 1.0281 - accuracy: 0.5312 - val_loss: 1.0305 - val_accuracy: 0.5300\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 66s 876ms/step - loss: 1.0270 - accuracy: 0.5292 - val_loss: 1.0139 - val_accuracy: 0.5500\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 63s 846ms/step - loss: 1.0147 - accuracy: 0.5350 - val_loss: 1.0144 - val_accuracy: 0.5500\n",
      "19/19 [==============================] - 5s 254ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "0.55\n",
      "0.55\n",
      "0.1456758294099136\n",
      "0.4002654051431344\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to have a third dimension\n",
    "X_tfidf = X_tfidf.reshape(X_tfidf.shape[0], X_tfidf.shape[1], 1)\n",
    "\n",
    "# Concatenate all feature types\n",
    "X_combined = np.hstack((X_tfidf, X_glove, X_word2vec, X_fasttext))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define GRU model\n",
    "model_combined = Sequential([\n",
    "    GRU(128, input_shape=(X_combined.shape[1],X_combined.shape[2]), activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_combined.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_combined.fit(X_train_combined, y_train, epochs=5, validation_data=(X_test_combined, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred_combined = np.argmax(model_combined.predict(X_test_combined), axis=-1)\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "recall_combined = recall_score(y_test, y_pred_combined, average='weighted')\n",
    "mcc_combined = matthews_corrcoef(y_test, y_pred_combined)\n",
    "f1_combined = f1_score(y_test, y_pred_combined, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46b03c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smelled scent hand sanitizers today someone pa...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey yankee yankeespr mlb wouldnt made sense pl...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diane wdunlap realdonaldtrump trump never clai...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brookbanktv one gift covid give appreciation s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>july medium bulletin novel coronavirusupdates ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_label\n",
       "0  smelled scent hand sanitizers today someone pa...            Fear\n",
       "1  hey yankee yankeespr mlb wouldnt made sense pl...             Joy\n",
       "2  diane wdunlap realdonaldtrump trump never clai...         Neutral\n",
       "3  brookbanktv one gift covid give appreciation s...         Neutral\n",
       "4  july medium bulletin novel coronavirusupdates ...         Neutral"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\My_Folders\\\\7th_sem\\\\IT350\\\\IT350_Project\\\\processed_covid19_tweets_copy.csv\")\n",
    "\n",
    "# Select the 10th and 14th columns\n",
    "selected_columns = df.iloc[:, [9, 13]]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "df = pd.DataFrame(selected_columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "df.head()  # You can adjust the number of rows to display if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6598d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
